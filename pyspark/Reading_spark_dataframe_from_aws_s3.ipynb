{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "copyrighted-attendance",
   "metadata": {},
   "source": [
    "download dependencies to read spark dataframe from s3 --> \n",
    "https://jar-download.com/artifacts/org.apache.hadoop/hadoop-aws\n",
    "add all file to jars folder at C:\\Apps\\spark-3.2.1-bin-hadoop3.2\\jars\n",
    "\n",
    "to check the right version of files to be downloaded search for hadoop in jar folder the version of those file written in the file name is the required version to be downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1868b",
   "metadata": {},
   "source": [
    "additional links for reading\n",
    "https://sparkbyexamples.com/spark/write-read-csv-file-from-s3-into-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.types import StructType,StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-jar:1.11.901,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\",\"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", 'org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below details can be accessed from IAM on aws or through the .aws credential file present in your local machine after logging into your aws account through saml2aws\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.session.token\",f\"{aws_session_token}\") \n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.security.token\",f\"{aws_security_token}\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", f\"{aws_access_key_id}\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", f\"{aws_secret_access_key}\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the s3 url of the folder where the pyspark dataframe parquet are present\n",
    "# replace the initial s3 with s3a so the s3 url should be changed from s3://{rest_of_the_url} to s3a://{rest_of_the_url}\n",
    "\n",
    "df = spark.read.parquet(r\"s3a://{rest_of_the_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('02').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to write spark dataframe to json\n",
    "\n",
    "df.coalesce(1).write.format('json').save(r'{file location}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-waters",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()\n",
    "current_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
